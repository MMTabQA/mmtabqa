First, scrape all the images, entity categories, page-views and replace the entity texts by the images IDs. Store all the relevant files in the `final_dataset_unfiltered` directory, and DON'T touch this anymore. All the code in this directory is responsible for creating final-dataset-filtered files and experiment-ready-dataset files. Run them in this order:
* `parse_categorywise_entity_annotatins\ copy.ipynb`: This file is responsible for parsing all the entities that are there in the dataset as included/excluded/seals/landscape/ignored based upon the category-wise include/exclude annotation that we had performed. IMPORTANT: WTQ uses differnt file for the annotations.
* `renewed_seals_collage_fetch.py`: This code fetches all the location collages and the seal collages corresponding to the location/seals links that were created in the previous file. Based upon this, the respective images and image paths get saved.
* `general_remove_rejected.py`: This code is responsible for removing the image tags of all the entities which are deemed to be not relevant. Such image IDs belong to the possible collage entities whose collages couldn't be made, rejected entities, etc. These files are stored in the `final_dataset_filtered` dataset.
* `questionExtraction.ipynb`: This file is responsible for creating the explicit and implicit questions.
* `create_experiment_ready_dataset.py`: This file is responsible for pruning the content of the images in the columns of the table to b/w 30% to 75%.
* `fixing_explicit_questions.ipynb`: This file is responsible for adding some extra information in the explicit image data files.